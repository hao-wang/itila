{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /Users/hao/anaconda/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sf in module scipy.stats._distn_infrastructure:\n",
      "\n",
      "sf(k, *args, **kwds) method of scipy.stats._discrete_distns.binom_gen instance\n",
      "    Survival function (1 - `cdf`) at k of the given RV.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    k : array_like\n",
      "        Quantiles.\n",
      "    arg1, arg2, arg3,... : array_like\n",
      "        The shape parameter(s) for the distribution (see docstring of the\n",
      "        instance object for more information).\n",
      "    loc : array_like, optional\n",
      "        Location parameter (default=0).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    sf : array_like\n",
      "        Survival function evaluated at k.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(binom.sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many repetitions are needed for $10^{-15}$ error probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -0.6997556141017829 0.19999999999999996\n",
      "3 -0.9840714694959846 0.10400000000000002\n",
      "5 -1.2385619699585548 0.057920000000000006\n",
      "7 -1.4786423621092328 0.033344000000000006\n",
      "9 -1.71007526093519 0.01958144000000001\n",
      "11 -1.9356905139715201 0.011654205440000008\n",
      "13 -2.15710283541803 0.007003561164800004\n",
      "15 -2.375326540079364 0.0042397497098240035\n",
      "17 -2.591043065088979 0.002581462836838403\n",
      "19 -2.804733565809475 0.0015791205491671057\n",
      "21 -3.0167510807349425 0.000969696438262957\n",
      "23 -3.2273626884284 0.0005973937086924227\n",
      "25 -3.436775542368577 0.00036904803455582816\n",
      "27 -3.6451536873751995 0.0002285276197025393\n",
      "29 -3.8526293315564324 0.00014180644939308117\n",
      "31 -4.059310640611395 8.815495202829619e-05\n",
      "33 -4.265287272277817 5.48910236621296e-05\n",
      "35 -4.470634397126805 3.42282540417343e-05\n",
      "37 -4.675415678581842 2.137141961126614e-05\n",
      "39 -4.879685520711574 1.335958173459544e-05\n",
      "41 -5.0834907903459845 8.360194899552932e-06\n",
      "43 -5.286872154952239 5.2367684578501736e-06\n",
      "45 -5.48986513509394 3.283207192494273e-06\n",
      "47 -5.6925009417846475 2.0601079654888367e-06\n",
      "49 -5.894807149583645 1.2936324498987639e-06\n",
      "51 -6.09680824275511 8.128990065206714e-07\n",
      "53 -6.298526062255578 5.111463220618058e-07\n",
      "55 -6.499980174461729 3.216009321202375e-07\n",
      "57 -6.701188177568736 2.0245811558553727e-07\n",
      "59 -6.902165957921927 1.2752139235819458e-07\n",
      "61 -7.102927905813073 8.036121454045366e-08\n",
      "63 -7.303487098216299 5.066551547586336e-08\n",
      "65 -7.503855454375138 3.195722506517137e-08\n",
      "67 -7.7040438689522155 2.016533292752309e-08\n",
      "69 -7.904062326524137 1.2729504473664903e-08\n",
      "71 -8.103920000479112 8.03855898048808e-09\n",
      "73 -8.303625338804691 5.078051158127592e-09\n",
      "75 -8.503186138801308 3.208930543794047e-09\n",
      "77 -8.702609612397284 2.028433313688656e-09\n",
      "79 -8.901902443451876 1.282601217540016e-09\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 80, 2):\n",
    "    err = binom.sf(i//2, n=i, p=0.2)\n",
    "    print(i, np.log(err)/2.3, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which coins to have to win a lottery at 99%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9999999999999999\n",
      "11 0.9999999999999999\n",
      "21 0.9999999999999999\n",
      "31 0.9999999999999999\n",
      "41 0.9999999999972475\n",
      "51 0.9999999873642345\n",
      "61 0.9999923332543739\n",
      "71 0.9991396722260117\n",
      "81 0.9769033260805924\n",
      "91 0.8141681145739925\n",
      "101 0.43180013567471304\n",
      "111 0.11385744267697884\n",
      "121 0.013427804717195993\n",
      "131 0.0006949694927738715\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 1000, 10):\n",
    "    prob = binom.sf(k, n=1000, p=0.1)\n",
    "    print(k, prob)\n",
    "    if prob < 0.01:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10404663256012281"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9*0.001*(np.log10(1000)/np.log10(2))+0.99*(np.log10(1/0.99)/np.log10(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45814536593707755"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(2.5)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_divergence = keras.losses.kullback_leibler_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77695024"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(0.1, [0.4, 0.6]).numpy()+kl_divergence(0.9, [0.6, 0.4]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.29957324"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(0.1, [0.4, 0.5]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3178053830347946"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(24.)*.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = np.random.rand(3, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79519495],\n",
       "       [0.63797495],\n",
       "       [0.66146343],\n",
       "       [0.41057597]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(0, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = time-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 20)          440       \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3*w1**2 + 2*w1*w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, (w1, w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd5bdf3e0b8>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD4CAYAAADM6gxlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeKklEQVR4nO3dd3xV9f3H8dcnIRBGwgx7gwKyIYCiaJ0oHbgVwYmioNXWibb+rFpXrbaOqqVuQQS3FVq31mJFIGHI3mGGhDACISHj+/sjFw0xwM3Nvffc8X4+Hj5y7kjO23MP75x877nfY845REQkeiR4HUBERKpHxS0iEmVU3CIiUUbFLSISZVTcIiJRplaoV9CsWTPXsWPHUK9GRCSmzJs3L9c5l1bVYyEv7o4dOzJ37txQr0ZEJKaY2fpDPaahEhGRKKPiFhGJMipuEZEoo+IWEYkyKm4RkSij4hYRiTJHLG4zO9HMPvMtJ5jZPWY2xswuD308ERGp7IjF7Zz7D1DXd/MSYItzbjJwnJm1C2U4EZFoVFbmeGDGEjbkFYTk5/s7VLLf93UEsNS3vBI4raonm9k4M5trZnNzcnJqGFFEJLo89fkq/vH1Wmatyg3Jz6/uGHczYIdvuRBoWdWTnHOTnHPpzrn0tLQqP7EpIhKT/rMih79+toJz+7fhokGhGZSobnHnAPV8yynA9uDGERGJXpt37uOmNzI5unkKD5zTGzMLyXqqW9wzgT6+5aOBT4MbR0QkOu0vKWPClAyKSx3PjhlA3dqJIVuXP2eV9Aa6mFkv4A2gs5ldBcxyzq0JWTIRkSjy4MylzN+wkz+d34fOaQ1Cuq4jzg7onFsEVByouSt0cUREos8HCzbz8jfrGHtCJ0b0bhXy9ekDOCIiNbBqWz4T315IeofGTDyre1jWqeIWEQnQ3qISrpucQb3aiTx9yQCSEsNTqSG/kIKISCxyznHnO4tYk7OHyWOH0LJhctjWrSNuEZEAvPq/9XywYDO3nNGNoV2bhXXdKm4RkWrKyNrBH2cs4dTuzRl/Upewr1/FLSJSDXl793PDlAxapCbz+IX9SEgIzYdsDkdj3CIifiotc9z0Ria5e/fzzvihNKyX5EkOHXGLiPjpyc9W8vXKXO79VU96tWnoWQ4Vt4iIH75cvo0nP1/JeQPacnGIJo/yl4pbROQINu3cx2+mzadbixT+eHavkE0e5S8Vt4jIYRSVlDJhSgalpY5nxwwM6eRR/tKbkyIih/HAjKUs2LCT58YMoFOz+l7HAXTELSJySO/P38Sr/1vPNcM6cWav0E8e5S8Vt4hIFVZm5zPx7UUM6tiY288Mz+RR/lJxi4hUsqeohOsmz6N+nVphnTzKX5GVRkTEY845Jr69kLW5e3lqVH9apIZv8ih/qbhFRCp4+Zt1fLhwC7cO78ZxXZp6HadKKm4REZ9563fwwIylnNajOdedGP7Jo/yl4hYRAbbvKeKG1zNo1SiZxy7wZvIof+k8bhGJe+WTR81nu8eTR/lLR9wiEvee+HQF/12Vy/0jvZ08yl8qbhGJa18s38aTn6/igoFtuWhQe6/j+EXFLSJxa+OOAn47bT49WqVy/9m9vI7jNxW3iMSlgyaPGj2A5CTvJ4/yl96cFJG4dP+HS1i4cRd/v3QgHSNk8ih/6YhbROLOe5mbmPxtFtee2JnhPVt6HafaVNwiEldWZOdz5zuLGNypCbcN7+Z1nICouEUkbhw0edSo/tSKsMmj/KUxbhGJC8457nhrIeu3FzDl6iE0j8DJo/wVnb9uRESq6cVZ65ixaAu3De/GsZ0jc/Iof6m4RSTmzV2Xx0Mzl3L6MS249sTOXsepsYCK28zqmdn9ZnaOmT1sZqnBDiYiEgy5e4q4/vUM2jSuy58v6Ov5FdqDIdAj7uFArnPuXWADcGrwIomIBEf55FGZ7Cwo5pnRA2hYN7Inj/JXoMX9HTDWzLoCKcBHwYskIhIcf/lkBbNWbef+kb3o2TryJ4/yV0DF7ZzbBDwBTAKynXMFFR83s3FmNtfM5ubk5AQhpohI9Xy+LJunv1jFheltuXBQO6/jBFWgY9ydgbbAWcDlZnZSxcedc5Occ+nOufS0tLQgxBQR8d+GvAJ+O20Bx7RK5b6R0TN5lL8CHSrpB+Q554qAvwL9gxVIRKQmCovLJ48qc45nx0TX5FH+CrS4ZwLtzGwE0A14JXiRREQCd9+HS1i0aRePX9iPDk2ja/IofwX0yUnnXCFwu+/mzODFEREJ3DsZG3l9dhbXndSF049p4XWckNEHcEQkJizbupu73l3EsZ2bcOsZR3sdJ6RU3CIS9fILixk/OYPU5CSejOLJo/ylSaZEJKo557j9rYVk5RUw9ZpjaZ4SvZNH+Su2fy2JSMx74b9r+df3W7njzG4M7tTE6zhhoeIWkag1Z10eD/1rGcN7tuCaYdE/eZS/VNwiEpVy8ou4fkoG7RrX5dEYmTzKXxrjFpGoU1Jaxo1TM9m1r5iXrxxManJsTB7lLxW3iESdxz9Zwf/WbOfR8/twTOv4m1VaQyUiElU+XZLNM1+u5uJB7bggPbYmj/KXiltEokbW9gJunj6fnq1T+cOvenodxzMqbhGJCoXFpUx4fR4Az44eGJOTR/lLY9wiEhXu/edivt+0m+cvS6d903pex/GUjrhFJOK9NW8jU7/bwISfdeG0GJ48yl8qbhGJaEu37OZ37y7iuM5Nufn02J48yl8qbhGJWLsLixk/eR4N68bH5FH+0hi3iEQk5xy3vbmADTv28ca4Y0lLqeN1pIihX18iEpH+8fUaPlqczZ1ndWdQx/iYPMpfKm4RiTiz12znkX8v56xeLRl7Qiev40QcFbeIRJRt+YXcMDWT9k3q8afz+8TV5FH+0hi3iESMktIyfv16JvmFxbw2djApcTZ5lL9U3CISMf788Qpmr83jsQv60r1l/E0e5S8NlYhIRPhkSTbPfbWaUYPbc97Atl7HiWgqbhHx3Prte7l5+nx6tUnlnl8e43WciKfiFhFPFRaXMn5yBglmcT95lL80xi0inrrn/cUs2bKbF69Ip12T+J48yl864hYRz0yfu4Fpczdw/cldOKW7Jo/yl4pbRDyxePMu7n7ve4Z2acrNp3fzOk5UUXGLSNjt2lfM+MkZNKpXPnlUYoI+ZFMdGuMWkbAqK3PcMn0+m3fuY9q1x9KsgSaPqi4dcYtIWD33n9V8unQbv/t5DwZ20ORRgVBxi0jYfLMqlz9/tJxf9GnFFUM7eh0naqm4RSQstu4q5NdTM+mc1oBHztPkUTUR0Bi3lW/xy4FtwALn3KagphKRmLK/pIwJU+axr7iUaWMGUL+O3l6riUC33sPAq865xcEMIyKx6aF/LSUjaydPjepP1+YpXseJetUeKjGzocAQ4BQze8jMalfxnHFmNtfM5ubk5AQjp4hEqQ8XbualWeu48viO/LJva6/jxIRAxrhHAi86554CmgA3VH6Cc26Scy7dOZeelpZW04wiEqVWbcvnjrcWMqB9I+48q4fXcWJGIMWdDOz2LX8I9ApeHBGJFXuLSrhucgbJSYn8bfQAatfSuRDBEsiW/C/Q37ecBMwJXhwRiQXOOSa+s4g1OXt4clR/WjWs63WkmFLt4nbOvQnUN7NzgA7AC0FPJSJR7ZVv1vHPBZu55YxuHN+1mddxYk5AZ5U4524NdhARiQ3z1u/gjzOWclqP5ow/qYvXcWKSBp1EJGhy9xRx/ZQMWjeqy2MX9CNBk0eFhM6CF5GgKC1z3Dg1kx0F+3lnwlAa1tMV2kNFxS0iQfH4J8v5ZvV2/nR+H3q2buh1nJimoRIRqbFPl2Tzty9Wc1F6Oy5Mb+d1nJin4haRGsnaXsBvp8+nZ+tU7h3Z0+s4cUHFLSIBKywuZfyUeRjoCu1hpDFuEQnYPe8vZvHm3bxweTrtm+oK7eGiI24RCci0OVlMm7uBG07uyqk9dIX2cFJxi0i1fb9pF3e/v5jjuzblt6cf7XWcuKPiFpFq2VVQzPgp82havzZPXqwrtHtBY9wi4reyMsfN0+ezdVch0649jqa6QrsndMQtIn579qvVfLZsG7//+TEMaN/Y6zhxS8UtIn7578pcHvt4Ob/s25rLjuvgdZy4puIWkSPasmsfN75RfoX2h8/trSu0e0zFLSKHVX6F9gyKikt5bsxAXaE9AugVEJHDenDmUjKzdvL0Jf3p2ryB13EEHXGLyGF8sGAzL3+zjquO78Qv+ugK7ZFCxS0iVVqZnc/EtxeS3qExd47o7nUcqUDFLSI/saeohOsmz6Ne7USevmQASYmqikiiMW4ROYhzjjveXsja3L1MvnoILRsmex1JKtGvURE5yEuz1jFj4RZuG96doV10hfZIpOIWkR/MWZfHgzOXclqPFlx7Ymev48ghqLhFBIBtuwuZMCWDto3r8tiFfXWF9gimMW4Robi0jOtfzyC/sJjXxg6mYV1doT2SqbhFhIdmLmPOuh08cXE/urdM9TqOHIGGSkTi3PvzN/HirLVcMbQjI/u18TqO+EHFLRLHlm/NZ+Lbi0jv0Ji7RvTwOo74ScUtEqd2FxZz3eR51K9Ti7+NHkDtWqqDaKExbpE4VFbmuGX6ArLyCnj96iG0SNWHbKKJfsWKxKFnv1rNJ0uyuWtED4Z0bup1HKkmFbdInPl6ZQ6PfbycX/RpxVXHd/Q6jgRAxS0SRzbuKODGqZl0bd6AR87royvZRKmAi9vMupvZjGCGEZHQKSwuZcKUDIpLna5kE+UCeuXMrA5wBlA/uHFEJFTu/ediFm7cxXNjBtI5TVeyiWaBHnFfCTx/qAfNbJyZzTWzuTk5OQGuQkSCZdqcLKZ+t4HxP+vCmb1aeh1HaqjaxW1mpwFfO+cKDvUc59wk51y6cy49LS2tRgFFpGYWbtzJ3e8v5oSuzbj1jG5ex5EgCGSo5Bqghe9NjX5m9jvn3APBjSUiwZC3dz/jJ2fQrH5tnri4H4ma8S8mVLu4nXMXHVg2sy9V2iKRqbTMcdMbmeTkF/HmdcfRtEEdryNJkOhtZZEY9ZdPVvD1ylweOrc3fds18jqOBFGNzuN2zv0sSDlEJIg+WZLN01+s4sL0tlw8qJ3XcSTI9AEckRizJmcPN0+bT682qdw3spc+ZBODVNwiMSS/sJhxr82jVqLx7OiBJCcleh1JQkBj3CIx4sCMf2ty9jB57BDaNanndSQJER1xi8SIv32xio99M/4N7drM6zgSQipukRjw+bJsHv90BSP7tWbsCZ28jiMhpuIWiXJrc/dy0xvz6dEylYfP1Yx/8UDFLRLF9hSVMO7VuSQmGH+/dCB1a+vNyHig4haJUs45bp2+gNU5e3h61AC9GRlHVNwiUeqZL1fz78VbufOsHpxwlN6MjCcqbpEo9MXybfz54+X8qm9rrh6mNyPjjYpbJMqsy93LTVMz6d4yVZcfi1MqbpEosreohHGvzSUhwZikNyPjlopbJEo457jtrQWs2raHp0b115uRcUzFLRIl/vbFKmYu2sodZ3Zn2FG6slQ8U3GLRIGPF2/lzx+XfzJy3ImdvY4jHlNxi0S4ZVt389tp8+nbtqHejBRAxS0S0fL27ufqV+ZSv04t/n5puqZpFUDTuopErOLSMsZPnse2/CKmX3scLRsmex1JIoSOuEUi1B8+WMzstXk8cl5v+umakVKBilskAr327XqmzM7i2pM6c07/tl7HkQij4haJMN+szuXeDxZzSvfm3D68u9dxJAKpuEUiSNb2AiZMyaBjs/o8cXE/EhN0Bon8lIpbJELkFxZz9atzcA6evyydlOQkryNJhNJZJSIRoKS0jF9PzWR1zl5euXIwHZvV9zqSRDAdcYt4zDnHfR8u4cvlOdw/spfm1pYjUnGLeOylWet49X/rGXdiZy4Z0t7rOBIFVNwiHvp0STb3z1jC8J4tmHimziAR/6i4RTzy/aZd3PhGJr3bNOSvF/UnQWeQiJ9U3CIe2LqrkLGvzKFR3SSevyxdF0SQatFZJSJhtreohLGvzGFvUSlvXncczVM1B4lUj464RcKopLSMG6dmsnTLbp66pD89WqV6HUmiULWL28xSzOxNM1tjZs+EIpRILHLO8fv3vuezZdu4b2QvTu7W3OtIEqUCGSo5FrgCcECmmQ1yzs0JaiqRGPTXT1fyxpwN3HByV8Yc28HrOBLFql3czrlPDiyb2ffA1srPMbNxwDiA9u11XqrI67OzeOKzlVwwsC23nHG013EkygU8xm1mKUCWc25D5cecc5Occ+nOufS0NF3UVOLbJ0uy+f17i/hZtzQePLe3Lj0mNVaTNycvBf4vWEFEYtG89Xnc8HoGvds05JnRA0hK1PkAUnMB7UVmdjbwnnMu38xaBDeSSGxYmZ3P2Ffm0qphMi9eMYh6tXX2rQRHIGeVTAD+AnxgZguBnwc9lUiUy9pewOjnZ5OUmMArVw2maYM6XkeSGBLIm5PPADoNUOQQtu4qZPQL37K/tIxp446jQ1NN0SrBpQE3kSDavqeIMS/MZsfeYl65cjDdWqZ4HUlikIpbJEh2FxZz2YvfsSGvgBcuT6evrswuIaLiFgmCvUUlXPXSHFZk5/PcpQMZ0rmp15Ekhqm4RWpoT1EJV7z0HZkbdvLExf31UXYJOZ2fJFIDe4pKuOLFA6XdjxG9W3kdSeKAilskQPmFxVz+4ncs2LiLp0b1V2lL2Ki4RQKw21faizbu4ulR/TlLpS1hpOIWqabcPUVc8dJ3LNuSz9OXDODMXi29jiRxRsUtUg0bdxRw2QvfsWnnPiZdNpBTumvGBwk/FbeIn1Zm53PpC9+xd38Jk68ewqCOTbyOJHFKxS3ih8ysHVz58hySEhOYfu1xuuSYeErncYscwcxFW7h40rekJifx1nUqbfGejrhFDsE5xzNfrubRj5YzsENjJl06ULP8SURQcYtUYX9JGXe+s4i3MzYysl9rHjmvD8lJiV7HEgFU3CI/sXnnPq5/PYPMrJ385rSjuOnUo3S5MYkoKm6RCmatyuXXUzPZX1LGM6MH6NOQEpFU3CJAWZnj2a9W89jHy+mS1oDnLh1Il7QGXscSqZKKW+Lepp37uGX6fL5dk8ev+rbmoXN7U7+O/mlI5NLeKXHLOcf78zdz9/vfU1bmePT8Ppw/sK3GsyXiqbglLm3dVcgfPljMvxdvJb1DYx6/sB/tm9bzOpaIX1TcEldKyxyTv13Pox8tp7i0jNvP7Ma4YZ2plajPokn0UHFL3Ji3Po/7/rmEBRt3MeyoZvzx7F66ArtEJRW3xLy1uXt55F/L+PfirTRPqcNfL+rHyH6tNZYtUUvFLTEra3sBz361mjfnbqBOrQRuPv1orh7WiXq1tdtLdNMeLDFnZXY+z3y5mg8WbCbRjFGD23PjqUeRlqJ5RiQ2qLglJhSXlvHpkmwmz17PrFXbqZuUyJVDO3LNiZ1pkZrsdTyRoFJxS1RbkZ3PB/M38+a8DWTvLqJNo7rcesbRXDKkA03q1/Y6nkhIqLglqjjnWJ2zh48WZ/PB/M0sz84nwWDYUWk8cHYHTu7enMQEvekosU3FLRFvd2Exs9fk8eXybXy5PIdNO/cBkN6hMff+qicjerfS+LXEFRW3RJTSMkdWXgHzN+xg7rodzFu/g+XZ+TgH9WsnMrRrMyac3IWTuzWndaO6XscV8YSKWzyxv6SMTTv3kZVXwNqcPSzbms/Srfms2JrPvuJSABrUqUX/9o04q1crBnVqTHqHJtSupU84iqi4Jaicc+QXlZCbX0ROfhE5e4rKl/cUsW13ERt3lJf1ll37KHM/fl/jekn0aJXKqMHt6d4qhV6tG9KtZYrGq0WqEFBxm9ktwDagoXPu6eBGkppwzlHmyoccSsscpc5RWlr+taSs7Mf7K/7nHCWljqKSMoqKSyksKaWouOzHr8WlFJWUUVhcRlFJKQX7S8kvLCG/sJjdhcXkF5awu7CY3ftK2FNUQmnFRvaplWA0bVCbNo3qMrhTE9o1qUd7338dm9YjLaWOPsko4qdqF7eZnQA0dc49ZmZ3m9kQ59zsYAebPmcDk75eA5SXEcBBdeD4yX2Vn+dcxae7n973036pcl3uh3W5Ku6r6ucd6XmHWUeFUFX8uAr/bz/eWeYoL2hfEYdSUqKRnJRIanISKcm1SE1OomVqMke3SPnhdmrdWqSl1CGtQXL515Q6NKqbRIKOnkWCIpAj7hHAUt/yEt/tg4rbzMYB4wDat28fULDG9WvTrUVKhR960JcD66nivoOfV/Eozn6yAOa7YQfdRxX3VfG8Kn5g5fUffN+RnvfTYvPne80gMSGBxATfVzNqJRoJZtRKMBISDv6amGBVPic5KZE6tRIO+pqclECdWj9+1dCFiPcCKe5mwA7fciHQsvITnHOTgEkA6enpAR0Cnn5MC04/pkUg3yoiEtMCeYs+Bzgw43wKsD14cURE5EgCKe6ZQB/f8jHAv4MXR0REjqTaxe2cmwUUmtlVwE7n3H+CH0tERA4loNMBnXN/DHYQERHxjz6GJiISZVTcIiJRRsUtIhJlVNwiIlHGXFWf+w7mCsxygPUBfnszIDeIcYJFuaonUnNB5GZTruqJxVwdnHNpVT0Q8uKuCTOb65xL9zpHZcpVPZGaCyI3m3JVT7zl0lCJiEiUUXGLiESZSC/uSV4HOATlqp5IzQWRm025qieuckX0GLeIiPxUpB9xi4hIJSpuEZEoo+IWEYkyEVHcZlbXzO4ys4kV7jvad03LW8zs6Cq+5xYzu9TMbghTxn+YWZaZrTOzPDNrUOnxjma22cy2mtlp4chUYd1P+tb7k7nRzexqM7vSzG4zs7C93mZ2qpl9bWZrzOysKh5PMbNVvtxXhilTlfvMkfa1EGdKMbM3fdvpmSoeP+RrG4Zsx/vWvcXMule438vtNczMcn3/DrPN7JpKj7/ry/yPMOU50cw+8y0nmNk9ZjbGzC6v4rlDff8O7zCz5jVasXMuIv4DzgD+UOH2v4AGQB3gnUrPPQF40Ld8NzAkxNms4jqAv1TxnDuAJA+2WxvgykM81hF41bd8GXBRGHOd4/t6KrC4iscnAE3CmOeQ+8zh9rUw5DodqE/5VaWWA4P8eW3DlO13+E5gqHS/l9trCD+eVHEn0LzCY4OA4R5sp298X8cA43zLzwHtKj3vK1+XtAeerck6I+KI22f/gQUzqwt0cc7tcc4VAZ3MrOLc4VVdsDhkXLnZvmx9gAUVHzez2sBwYL2ZjQ5lliqcAtxtZh+aWbNKj50BrPQtLybE26ki59y7vsU5wJYqnnISsMTMbglTpCr3GT/2tZByzn3inNvrnCsAvge2Vnj4cK9tSPmOCM8G1pjZ6RXu93p7zXa+FgTSnHPbKjx8MvC8mb1iZvWq+PZQOdBdFfexlcAPf3n7/jIp8XVJFjCsJisM2wY/wMzuAir/efUesLPC7cbA7gq3S4A0fiyAI16wOBQZnXPvAb8Enq34gHNuP3CKmbUFZpjZHOfcimBmOlwuoAvwG+Bxyo+sDwjpdjpSLt/2GgE8XPl7nHMXmVlj4F0zy3TOfR6KbBUcalscaV8LCzNLAbKccxsO3Oece83MJlP1axtSvkIcZGY9gbfN7Fjn3E4iZ3t1BNZWvM859yczexx4BJgI/F84M3H4f28VH4Py7RiwsBe3c+7Bqu43s59VuLkdSK5wux4HF3tIL1h8qIw+TZxzeYf4vo1m9gDQCwh6cR8h11/M7I1K9+Xw4w4Ssgs7H+Y1bQbUd85VznXg+3aY2e2UD2OEurgPtc8caV8Ll0upomh8R5dVvbZh4ZxbbGYvAp2BDCJne50DTK98p3OuxMzuAF4Kf6TD9lLFxwCKarKiSBoq+YHvT7D1ZlbPzJKBDc65fWbW0MwMjy5YbGZHUaGQfW8sJfqWzXd3XeDbcOSpuF7fcM2cSrk+Anr6nhrWCzubWX1ghHPuBTOrZWZNzSzZ93pW3F6NgS/DEKnyPvORmTU81L4Whjw/MLOzKf8LJd/MWvj+EqnytQ1jJqtwcz+wNFK2l08b59wmK9ewUuYU4L8eZKq4jx0NfGpmiWaW4pxbie8Xnpl1pob7fER8ctI3RnY70J/ywf0dZtYLOJ/y30zvO+eW+I46HnTOLTSz3wObgUbOucfDlPNW4DXnXLbv9qOUvwB5wJPAW8D/XBgvoGxmb1L+J1gm5W9E7j2Qyzk3w8rPoNhH+Z9tDzvnSsOQqQ7lvzRSgTKgITCA8j/1HTAD+IDy7ZXpnPsw1Jl8uX7YZ4DPgInOuVFV7WvhyOPLNAG4jfKjs9rAK8BI59yJVb22Ycx1AXAD5a/T55S/jp5vL1+2lsAlzrnHzaxvhVyzgO8of6/g5TDt670pL+yzKB/fvh9YRXm3vmBmvwCGOefuMLNTgHTKD+4mOecCHl6KiOIWERH/ReRQiYiIHJqKW0Qkyqi4RUSijIpbRCTKqLhFRKKMiltEJMqouEVEosz/A9WnN5IWOynMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10, 0.01)\n",
    "plt.plot(x, np.log(1+np.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_softplus(x):\n",
    "    exp = tf.exp(x)\n",
    "    def my_softplus_gradient(grad):\n",
    "        return grad/(1+tf.exp(-x))\n",
    "    \n",
    "    return tf.math.log(1+exp), my_softplus_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([100.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "    \n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = my_softplus(tf.Variable([10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999522881542902"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = (1+0.01)**1000\n",
    "(x-1)/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030301000000000133"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1+0.01)**3-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(tf.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.threhold = threshold\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            return X+tf.random.normal(X.shape(), stddev=self.stddev)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units=3, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name=\"weight\", \n",
    "                                      shape=[batch_input_shape[-1], self.units],\n",
    "                                      initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(name=\"bias\",\n",
    "                                    shape=[self.units],\n",
    "                                    initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \n",
    "                \"units\": self.units, \n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal((10000, 3), mean=1, stddev=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 3), dtype=float32, numpy=\n",
       "array([[-1.1809762,  1.1248097, -1.1597238],\n",
       "       [-5.6291895,  0.9532326,  4.1330967],\n",
       "       [ 4.219125 ,  5.805277 , -0.7516353],\n",
       "       ...,\n",
       "       [ 2.9167151,  2.1875405,  5.718116 ],\n",
       "       [ 4.3632903,  3.918015 ,  3.8658614],\n",
       "       [-1.8236792,  4.937236 ,  1.7042985]], dtype=float32)>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x @ tf.constant([2., .1, .5])[:, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 1), dtype=float32, numpy=\n",
       "array([[-2.8293333],\n",
       "       [-9.096507 ],\n",
       "       [ 8.64296  ],\n",
       "       ...,\n",
       "       [ 8.911242 ],\n",
       "       [11.051313 ],\n",
       "       [-2.3014855]], dtype=float32)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = x[:7000], x[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid = y[:7000], y[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = tf.data.Dataset.from_tensor_slices(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = tf.data.Dataset.from_tensor_slices(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    myDense(1, input_shape=[3])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_dense_6 (myDense)         (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 24.4681 - val_loss: 22.5362\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 21.5415 - val_loss: 19.7696\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 18.7926 - val_loss: 17.1860\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 16.3404 - val_loss: 14.8440\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 14.0490 - val_loss: 12.6886\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 11.9354 - val_loss: 10.6856\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 9.9905 - val_loss: 8.8643\n",
      "Epoch 8/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 8.2540 - val_loss: 7.2452\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 6.7121 - val_loss: 5.8057\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 5.3557 - val_loss: 4.5668\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 4.1580 - val_loss: 3.4751\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 3.1191 - val_loss: 2.5536\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 2.2332 - val_loss: 1.7762\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 1.5177 - val_loss: 1.1532\n",
      "Epoch 15/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.9484 - val_loss: 0.6797\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.5200 - val_loss: 0.3350\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2310 - val_loss: 0.1235\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0721 - val_loss: 0.0351\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0219 - val_loss: 0.0116\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.0055 - val_loss: 0.0013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd5be4f3630>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, validation_split=0.7, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit in module tensorflow.python.keras.engine.training:\n",
      "\n",
      "fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False)\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    Arguments:\n",
      "        x: Input data. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "          - A `tf.data` dataset. Should return a tuple\n",
      "            of either `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "            or `(inputs, targets, sample_weights)`.\n",
      "          A more detailed description of unpacking behavior for iterator types\n",
      "          (Dataset, generator, Sequence) is given below.\n",
      "        y: Target data. Like the input data `x`,\n",
      "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "          or `keras.utils.Sequence` instance, `y` should\n",
      "          not be specified (since targets will be obtained from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            Note that the progress bar is not particularly useful when\n",
      "            logged to a file, so verbose=2 is recommended when not running\n",
      "            interactively (eg, in a production environment).\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `tf.keras.callbacks`.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling. This argument is\n",
      "            not supported when `x` is a dataset, generator or\n",
      "           `keras.utils.Sequence` instance.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data. Thus, note the fact\n",
      "            that the validation loss of data provided using `validation_split`\n",
      "            or `validation_data` is not affected by regularization layers like\n",
      "            noise and dropuout.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      "              - dataset\n",
      "            For the first two cases, `batch_size` must be provided.\n",
      "            For the last case, `validation_steps` could be provided.\n",
      "            Note that `validation_data` does not support all the data types that\n",
      "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch'). This argument is ignored\n",
      "            when `x` is a generator. 'batch' is a special option for dealing\n",
      "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample. This\n",
      "            argument is not supported when `x` is a dataset, generator, or\n",
      "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "            as the third element of `x`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined. If x is a\n",
      "            `tf.data` dataset, and 'steps_per_epoch'\n",
      "            is None, the epoch will run until the input dataset is exhausted.\n",
      "            When passing an infinitely repeating dataset, you must specify the\n",
      "            `steps_per_epoch` argument. This argument is not supported with\n",
      "            array inputs.\n",
      "        validation_steps: Only relevant if `validation_data` is provided and\n",
      "            is a `tf.data` dataset. Total number of steps (batches of\n",
      "            samples) to draw before stopping when performing validation\n",
      "            at the end of every epoch. If 'validation_steps' is None, validation\n",
      "            will run until the `validation_data` dataset is exhausted. In the\n",
      "            case of an infinitely repeated dataset, it will run into an\n",
      "            infinite loop. If 'validation_steps' is specified and only part of\n",
      "            the dataset will be consumed, the evaluation will start from the\n",
      "            beginning of the dataset at each epoch. This ensures that the same\n",
      "            validation samples are used every time.\n",
      "        validation_batch_size: Integer or `None`.\n",
      "            Number of samples per validation batch.\n",
      "            If unspecified, will default to `batch_size`.\n",
      "            Do not specify the `validation_batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      "            If an integer, specifies how many training epochs to run before a\n",
      "            new validation run is performed, e.g. `validation_freq=2` runs\n",
      "            validation every 2 epochs. If a Container, specifies the epochs on\n",
      "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1. If 0, will execute the generator on the main\n",
      "            thread.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "    \n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "      yield not only features (x) but optionally targets (y) and sample weights.\n",
      "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "      second and third elements will be used for y and sample_weight\n",
      "      respectively. Any other type provided will be wrapped in a length one\n",
      "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "      should still adhere to the top-level tuple structure.\n",
      "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "      features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the namedtuple. The reason is that\n",
      "      it behaves like both an ordered datatype (tuple) and a mapping\n",
      "      datatype (dict). So given a namedtuple of the form:\n",
      "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "      it is ambiguous whether to reverse the order of the elements when\n",
      "      interpreting the value. Even worse is a tuple of the form:\n",
      "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "      and sample_weight or passed through as a single element to `x`. As a\n",
      "      result the data processing code will simply raise a ValueError if it\n",
      "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: 1. If the model was never compiled or,\n",
      "        2. If `model.fit` is  wrapped in `tf.function`.\n",
      "    \n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.models.Model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.1801 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0760 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0336 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0090 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 4.9224e-04 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.1443e-05 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.0395e-05 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 0s 936us/step - loss: 9.9455e-06 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 0s 924us/step - loss: 1.1028e-05 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 0s 870us/step - loss: 9.6792e-06 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd5bef7de10>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=[x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.8296778],\n",
       "       [-9.097635 ],\n",
       "       [ 8.641678 ],\n",
       "       ...,\n",
       "       [ 8.909695 ],\n",
       "       [11.049713 ],\n",
       "       [-2.3030035]], dtype=float32)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 1), dtype=float32, numpy=\n",
       "array([[-2.8293333],\n",
       "       [-9.096507 ],\n",
       "       [ 8.64296  ],\n",
       "       ...,\n",
       "       [ 8.911242 ],\n",
       "       [11.051313 ],\n",
       "       [-2.3014855]], dtype=float32)>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Use layer_normalization to fight unstable gradients.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        self.state_size=units\n",
    "        self.output_size=units\n",
    "        self.activation=tf.keras.activations.get(activation)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    tf.keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
